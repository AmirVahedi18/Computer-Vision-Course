{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c455982e",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fb8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse \n",
    "import imutils \n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af91d7",
   "metadata": {},
   "source": [
    "# Centroid Tracker Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663a2b9c-3cb5-4d44-ba69-5f92abef871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentroidTracker():\n",
    "    def __init__(self, maxDisappeared=50):\n",
    "        self.nextObjectID = 0\n",
    "        self.objects = OrderedDict()\n",
    "        self.disappeared = OrderedDict()\n",
    "        self.maxDisappeared = maxDisappeared\n",
    "\n",
    "    def register(self, centroid):\n",
    "        self.objects[self.nextObjectID] = centroid\n",
    "        self.disappeared[self.nextObjectID] = 0\n",
    "        self.nextObjectID += 1\n",
    "\n",
    "    def deregister(self, objectID):\n",
    "        del self.objects[objectID]\n",
    "        del self.disappeared[objectID]\n",
    "\n",
    "    def update(self, rects):\n",
    "        if len(rects) == 0:\n",
    "            for objectID in list(self.disappeared.keys()):\n",
    "                self.disappeared[objectID] += 1\n",
    "                if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                    self.deregister(objectID)\n",
    "            return self.objects\n",
    "\n",
    "        inputCentroids = np.zeros((len(rects), 2), dtype='int')\n",
    "\n",
    "        for (i, (startX, startY, endX, endY)) in enumerate(rects):\n",
    "            cX = int((startX + endX) / 2)\n",
    "            cY = int((startY + endY) / 2)\n",
    "            inputCentroids[i] = (cX, cY)\n",
    "\n",
    "        if len(self.objects) == 0:\n",
    "            for i in range(0, len(inputCentroids)):\n",
    "                self.register(inputCentroids[i])\n",
    "        else:\n",
    "            objectIDs = list(self.objects.keys())\n",
    "            objectCentroids = list(self.objects.values())\n",
    "            D = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
    "            rows = D.min(axis=1).argsort()\n",
    "            cols = D.argmin(axis=1)[rows]\n",
    "            usedRows = set()\n",
    "            usedCols = set()\n",
    "\n",
    "            for (row, col) in zip(rows, cols):\n",
    "                if row in usedRows or col in usedCols:\n",
    "                    continue\n",
    "                objectID = objectIDs[row]\n",
    "                self.objects[objectID] = inputCentroids[col]\n",
    "                self.disappeared[objectID] = 0\n",
    "                usedRows.add(row)\n",
    "                usedCols.add(col)\n",
    "\n",
    "            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
    "            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
    "\n",
    "            if D.shape[0] >= D.shape[1]:\n",
    "                for row in unusedRows:\n",
    "                    objectID = objectIDs[row]\n",
    "                    self.disappeared[objectID] += 1\n",
    "                    if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                        self.deregister(objectID)\n",
    "            else:\n",
    "                for col in unusedCols:\n",
    "                    self.register(inputCentroids[col])\n",
    "\n",
    "        return self.objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5595ced",
   "metadata": {},
   "source": [
    "# Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23f1bbd4-46ab-47d4-9b5b-90db6d505168",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTOTXT_ADDRESS =  os.path.join(\"models\", \"deploy.prototxt\")\n",
    "MODEL_ADDRESS =  os.path.join(\"models\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "faceProto = os.path.join(\"models\", \"opencv_face_detector.pbtxt\")\n",
    "faceModel = os.path.join(\"models\", \"opencv_face_detector_uint8.pb\")\n",
    "\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "\n",
    "CONFIDENCE = 0.5\n",
    "\n",
    "# initialize our centroid tracker and frame dimensions\n",
    "(H, W) = (None, None)\n",
    "\n",
    "# load our serialized model from disk\n",
    "net = cv2.dnn.readNetFromCaffe(PROTOTXT_ADDRESS, MODEL_ADDRESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6e646",
   "metadata": {},
   "source": [
    "# Face Detection Using WebCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5869ec6b-3a5b-4498-b25c-f0320232b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fps(image, fps: int):\n",
    "    # Check if the image is grayscale or colored.\n",
    "    # If it's grayscale, set text color to white; if colored, set text color to green.\n",
    "    if len(np.shape(image)) < 3:\n",
    "        text_color = (255, 255, 255)  # White color for grayscale images.\n",
    "    else:\n",
    "        text_color = (0, 255, 0)  # Green color for colored images.\n",
    "\n",
    "    # Define the row size for the text placement.\n",
    "    row_size = 20 \n",
    "    # Define the left margin for the text placement.\n",
    "    left_margin = 24 \n",
    "\n",
    "    # Set the font size and thickness for the text.\n",
    "    font_size = 1\n",
    "    font_thickness = 2\n",
    "\n",
    "    # Format the FPS value into a string for displaying.\n",
    "    fps_text = \"FPS = {:.1f}\".format(fps)\n",
    "    # Set the text location on the image.\n",
    "    text_location = (left_margin, row_size)\n",
    "    \n",
    "    # Place the FPS text on the image.\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        fps_text,\n",
    "        text_location,\n",
    "        cv2.FONT_HERSHEY_PLAIN,\n",
    "        font_size,\n",
    "        text_color,\n",
    "        font_thickness,\n",
    "    )\n",
    "\n",
    "    # Return the modified image with the FPS text.\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51de9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for camera settings\n",
    "CAMERA_DEVICE_ID = 0  # ID for the camera device\n",
    "IMAGE_WIDTH = 800  # Width of the captured image\n",
    "IMAGE_HEIGHT = 600  # Height of the captured image\n",
    "fps = 0  # Initial Frames Per Second (FPS) value\n",
    "ct = CentroidTracker()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  # Ensures that the code only runs when executed directly\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(CAMERA_DEVICE_ID)  # Initialize video capture with the specified camera device ID\n",
    "\n",
    "        while True:  # Infinite loop to continuously capture frames\n",
    "\n",
    "            start_time = time.time()  # Record the start time to calculate FPS\n",
    "\n",
    "            _, frame = cap.read()  # Capture a single frame from the camera\n",
    "\n",
    "            frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))  # Resize the frame to the specified dimensions\n",
    "\n",
    "            if H is None or W is None:\n",
    "                (H, W) = frame.shape[:2]\n",
    "                \n",
    "            # construct a blob from the frame, pass it through the network,\n",
    "\t        # obtain our output predictions, and initialize the list of\n",
    "\t        # bounding box rectangles\n",
    "            blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H), (104.0, 177.0, 123.0))\n",
    "            faceNet.setInput(blob)\n",
    "            detections = faceNet.forward()\n",
    "            rects = []\n",
    "            \n",
    "            # loop over the detections\n",
    "            for i in range(0, detections.shape[2]):\n",
    "                # filter out weak detections by ensuring the predicted\n",
    "\t        \t# probability is greater than a minimum threshold\n",
    "                if detections[0, 0, i, 2] > CONFIDENCE:\n",
    "                    # compute the (x, y)-coordinates of the bounding box for\n",
    "\t        \t\t# the object, then update the bounding box rectangles list\n",
    "                    box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "                    rects.append(box.astype('int'))\n",
    "                    \n",
    "                    # draw a bounding box surrounding the object so we can\n",
    "\t        \t\t# visualize it\n",
    "                    (startX, startY, endX, endY) = box.astype('int')\n",
    "                    cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                    \n",
    "            # update our centroid tracker using the computed set of bounding\n",
    "\t        # box rectangles\n",
    "            objects = ct.update(rects)\n",
    "            \n",
    "            # loop over the tracked objects\n",
    "            for (objectID, centroid) in objects.items():\n",
    "                # draw both the ID of the object and the centroid of the\n",
    "\t        \t# object on the output frame\n",
    "                text = \"ID {}\".format(objectID)\n",
    "                cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "\n",
    "            # Display the frame with FPS overlay\n",
    "            cv2.imshow(\"frame\", visualize_fps(frame, fps)) \n",
    "\n",
    "            end_time = time.time()  # Record the end time to calculate FPS\n",
    "\n",
    "            # Calculate the time taken to process the frame\n",
    "            seconds = end_time - start_time \n",
    "            # Calculate FPS based on the time taken to process the frame\n",
    "            fps = 1.0 / seconds\n",
    "\n",
    "            # Break the loop if the 'Esc' key (ASCII 27) is pressed\n",
    "            if cv2.waitKey(33) == 27:  \n",
    "                break\n",
    "    except Exception as e:  # Handle exceptions that may occur\n",
    "        print(e)  # Print the exception message\n",
    "    finally:\n",
    "        cv2.destroyAllWindows()  # Close all OpenCV windows\n",
    "        cap.release()  # Release the camera resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ed670e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for video settings\n",
    "# Define the path to the video file using os.path.join\n",
    "CAMERA_DEVICE_ID = os.path.join(\"videos\", \"Einstein_1.mp4\")  # Path to the video file\n",
    "IMAGE_WIDTH = 1000  # Width of the displayed image\n",
    "IMAGE_HEIGHT = 600  # Height of the displayed image\n",
    "fps = 0  # Initial Frames Per Second (FPS) value\n",
    "FRAME_RATE = 30  # Desired frame rate\n",
    "DURATION = 1 / FRAME_RATE  # Duration of each frame\n",
    "\n",
    "if __name__ == \"__main__\":  # Ensures that the code only runs when executed directly\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(CAMERA_DEVICE_ID)  # Initialize video capture with the specified video file path\n",
    "\n",
    "        while True:  # Infinite loop to continuously capture frames\n",
    "            start_time = time.time()  # Record the start time to calculate FPS\n",
    "\n",
    "            _, frame = cap.read()  # Capture a single frame from the video\n",
    "            frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))  # Resize the frame to the specified dimensions\n",
    "\n",
    "\n",
    "            if H is None or W is None:\n",
    "                (H, W) = frame.shape[:2]\n",
    "                \n",
    "            # construct a blob from the frame, pass it through the network,\n",
    "\t        # obtain our output predictions, and initialize the list of\n",
    "\t        # bounding box rectangles\n",
    "            blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H), (104.0, 177.0, 123.0))\n",
    "            faceNet.setInput(blob)\n",
    "            detections = faceNet.forward()\n",
    "            rects = []\n",
    "            \n",
    "            # loop over the detections\n",
    "            for i in range(0, detections.shape[2]):\n",
    "                # filter out weak detections by ensuring the predicted\n",
    "\t        \t# probability is greater than a minimum threshold\n",
    "                if detections[0, 0, i, 2] > CONFIDENCE:\n",
    "                    # compute the (x, y)-coordinates of the bounding box for\n",
    "\t        \t\t# the object, then update the bounding box rectangles list\n",
    "                    box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "                    rects.append(box.astype('int'))\n",
    "                    \n",
    "                    # draw a bounding box surrounding the object so we can\n",
    "\t        \t\t# visualize it\n",
    "                    (startX, startY, endX, endY) = box.astype('int')\n",
    "                    cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                    \n",
    "            # update our centroid tracker using the computed set of bounding\n",
    "\t        # box rectangles\n",
    "            objects = ct.update(rects)\n",
    "            \n",
    "            # loop over the tracked objects\n",
    "            for (objectID, centroid) in objects.items():\n",
    "                # draw both the ID of the object and the centroid of the\n",
    "\t        \t# object on the output frame\n",
    "                text = \"ID {}\".format(objectID)\n",
    "                cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "\n",
    "            end_time = time.time()  # Record the end time to calculate FPS\n",
    "\n",
    "            # Calculate the time taken to process the frame\n",
    "            seconds = end_time - start_time\n",
    "\n",
    "            # If the processing time is less than the desired frame duration, wait for the remaining time\n",
    "            if seconds < DURATION:\n",
    "                time.sleep(DURATION - seconds)\n",
    "\n",
    "            # Recalculate the time taken to process the frame including sleep time\n",
    "            seconds = time.time() - start_time\n",
    "            # Calculate FPS based on the time taken to process the frame\n",
    "            fps = 1.0 / seconds\n",
    "\n",
    "            # Display the frame with FPS overlay\n",
    "            cv2.imshow(\"frame\", visualize_fps(frame, fps))\n",
    "\n",
    "            # Break the loop if the 'Esc' key (ASCII 27) is pressed\n",
    "            if cv2.waitKey(33) == 27:  \n",
    "                break\n",
    "    except Exception as e:  # Handle exceptions that may occur\n",
    "        print(e)  # Print the exception message\n",
    "    finally:\n",
    "        cv2.destroyAllWindows()  # Close all OpenCV windows\n",
    "        cap.release()  # Release the video capture resource\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
